{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwyczX6aOivDtPyi3QUaPl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DTLJP2fZlyWh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","import logging\n","import warnings\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from sklearn.model_selection import TimeSeriesSplit\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","#################################\n","# 1. SETUP LOGGING\n","#################################\n","logging.basicConfig(\n","    level=logging.DEBUG,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    handlers=[\n","        logging.FileHandler(\"llm_inference_debug.log\", mode=\"w\"),\n","        logging.StreamHandler()\n","    ]\n",")\n","\n","#################################\n","# 2. DATA LOADING & PREPROCESSING\n","#################################\n","def load_and_prepare_data():\n","    logging.info(\"Loading data...\")\n","    filepath_news = 'multimodal_S&P500_all.csv'\n","    filepath_num = 'merged_S&P500.csv'\n","\n","    data = pd.read_csv(filepath_news)\n","    data_num = pd.read_csv(filepath_num)\n","\n","    data['Date'] = pd.to_datetime(data['Date'])\n","    data_num['Date'] = pd.to_datetime(data_num['Date'])\n","\n","    merged_data = pd.merge(\n","        data,\n","        data_num[['Date', 'Daily_Return']],\n","        on='Date',\n","        how='inner'\n","    )\n","    if 'Unnamed: 0' in merged_data.columns:\n","        merged_data.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","    rolling_features = [\n","        'garch_cond_variance_lag1', 'garch_cond_volatility_lag1',\n","        'garch_residuals_lag1', 'rolling_cond_volatility_3_lag1',\n","        'rolling_cond_volatility_5_lag1'\n","    ]\n","    for col in rolling_features:\n","        if col in merged_data.columns:\n","            merged_data.drop(columns=[col], inplace=True)\n","\n","    merged_data.sort_values(by=\"Date\", inplace=True)\n","    merged_data.reset_index(drop=True, inplace=True)\n","\n","    filtered_data = merged_data[\n","        (merged_data['cleaned_content'].notnull()) &\n","        (merged_data['cleaned_content'] != \"\")\n","    ].reset_index(drop=True)\n","\n","    logging.info(f\"Full data shape after merge and filter: {filtered_data.shape}\")\n","    logging.debug(f\"Sample data:\\n{filtered_data.head(3)}\")\n","    return filtered_data\n","\n","#################################\n","# 3. ATTACH PREVIOUS DAY NEWS\n","#################################\n","def attach_prev_day_news(df):\n","    logging.info(\"Attaching previous day news...\")\n","    unique_dates = np.sort(df[\"Date\"].unique())\n","    prev_news_map = {}\n","\n","    for current_date in unique_dates:\n","        prev_rows = df[(df[\"Date\"] < current_date) & (df[\"cleaned_content\"] != \"\")]\n","        if prev_rows.empty:\n","            prev_news_map[current_date] = \"No previous news available\"\n","        else:\n","            last_date = prev_rows[\"Date\"].max()\n","            articles = prev_rows[prev_rows[\"Date\"] == last_date][\"cleaned_content\"]\n","            combined_articles = \" \".join(articles.astype(str).tolist())\n","            prev_news_map[current_date] = combined_articles\n","\n","    df[\"prev_day_news\"] = df[\"Date\"].map(prev_news_map)\n","    logging.debug(\"Finished attaching previous day news.\")\n","    return df\n","\n","#################################\n","# 4. TIME SERIES SPLIT\n","#################################\n","def get_time_series_folds(df, n_splits=5):\n","    tss = TimeSeriesSplit(n_splits=n_splits)\n","    folds = []\n","    fold_num = 1\n","    for train_idx, test_idx in tss.split(df):\n","        train_df = df.iloc[train_idx].reset_index(drop=True)\n","        test_df = df.iloc[test_idx].reset_index(drop=True)\n","        logging.info(f\"Fold {fold_num}: Train set shape: {train_df.shape}, Test set shape: {test_df.shape}\")\n","        logging.debug(f\"Fold {fold_num} - Test sample:\\n{test_df.head(3)}\")\n","        folds.append((train_df, test_df))\n","        fold_num += 1\n","    return folds\n","\n","#################################\n","# 5. LLM LOADING & PROMPT LOGIC\n","#################################\n","def load_llm_qwen(use_gpu=True):\n","    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n","    logging.info(f\"Loading Qwen model: {model_name} on GPU={use_gpu}\")\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        trust_remote_code=True,\n","        device_map=\"auto\" if use_gpu else None,\n","        torch_dtype=torch.float16 if use_gpu else torch.float32\n","    )\n","    text_gen = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        temperature=0.2,\n","        max_new_tokens=50\n","    )\n","    logging.info(\"LLM pipeline ready on GPU.\")\n","    return text_gen\n","\n","### 1â€‘Shot Prompt Version ###\n","def build_prompt_one_shot(row):\n","    example = (\n","        \"Example:\\n\"\n","        \"<Numerical Data>: \\\"Open: 4500; sentiment_volatility_lag1: 0; aggregate_sentiment_score_lag1: 2; \"\n","        \"Close_lag1: 4480; High_lag1: 4510; Volume_lag1: 345000000; Daily_Return_lag1: 0; Volatility_lag1: 0\\\"\\n\"\n","        \"<News Article>: Federal Reserve announces interest rate cut to boost economic growth.\\n\"\n","        \"</Answer/>: up\\n\\n\"\n","    )\n","\n","    numerical_data = \"; \".join([\n","        f\"{col}: {int(float(row[col]))}\" for col in [\n","            \"Open\", \"sentiment_volatility_lag1\", \"aggregate_sentiment_score_lag1\",\n","            \"Close_lag1\", \"High_lag1\", \"Volume_lag1\", \"Daily_Return_lag1\", \"Volatility_lag1\"\n","        ] if col in row and pd.notnull(row[col])\n","    ])\n","    news_article = str(row.get(\"prev_day_news\", \"No previous news available\"))\n","\n","    prompt = (\n","        example +\n","        \"Based on the numerical data and news article from yesterday, predict whether the stock will move 'up' or 'down' today.\\n\"\n","        \"Respond with exactly one word ('up' or 'down') on a new line. Do not echo any part of this prompt or the above example.\\n\"\n","        \"Your answer should appear immediately after the marker below.\\n\\n\"\n","        f\"<Numerical Data>: \\\"{numerical_data}\\\"\\n\"\n","        f\"<News Article>: {news_article}\\n\\n\"\n","        \"<<<ANSWER_START>>>\"\n","    )\n","    return prompt\n","\n","#################################\n","# ANSWER EXTRACTION AND PREDICTION\n","#################################\n","def extract_answer(model_output):\n","    \"\"\"\n","    Extracts 'up' or 'down' from the model's generated text.\n","    It first looks for the unique marker and extracts all text that follows,\n","    then applies a strict regex to capture exactly 'up' or 'down' at the end.\n","    \"\"\"\n","    marker = \"<<<ANSWER_START>>>\"\n","    if marker in model_output:\n","        answer_text = model_output.split(marker)[-1].strip()\n","    else:\n","        answer_text = model_output.strip()\n","\n","    match = re.search(r\"(up|down)\\s*$\", answer_text, re.IGNORECASE)\n","    if match:\n","        return match.group(1).lower()\n","    else:\n","        tokens = answer_text.split()\n","        if tokens and tokens[-1].lower() in [\"up\", \"down\"]:\n","            return tokens[-1].lower()\n","    return None\n","\n","def predict_movement(llm_pipeline, row, prompt_builder):\n","    prompt = prompt_builder(row)\n","    logging.debug(f\"Generated prompt:\\n{prompt}\")\n","\n","    try:\n","        output = llm_pipeline(prompt, max_new_tokens=50)[0]['generated_text']\n","        logging.debug(f\"Model raw output: {output}\")\n","\n","        # Remove any echoed prompt if present\n","        if output.startswith(prompt):\n","            cleaned_text = output[len(prompt):].strip()\n","        else:\n","            cleaned_text = output.strip()\n","        logging.debug(f\"Cleaned model output: {cleaned_text}\")\n","\n","        answer = extract_answer(cleaned_text)\n","        if answer == \"up\":\n","            logging.debug(\"Extracted answer: up\")\n","            return 1, \"up\"\n","        elif answer == \"down\":\n","            logging.debug(\"Extracted answer: down\")\n","            return 0, \"down\"\n","        else:\n","            logging.warning(\"Failed to extract a valid answer. Defaulting to 'up'.\")\n","            return 1, \"error\"\n","    except Exception as e:\n","        logging.error(f\"Error during LLM inference: {e}\")\n","        return 1, \"error\"\n","\n","#################################\n","# FOLD-WISE INFERENCE & REAL-TIME FILE WRITING\n","#################################\n","def main():\n","    logging.info(\"Starting fold-wise inference for unique dates in each test set...\")\n","    df = load_and_prepare_data()\n","    df = attach_prev_day_news(df)\n","    folds = get_time_series_folds(df, n_splits=5)\n","\n","    # To use one-shot, set: prompt_builder = build_prompt_1_shot\n","    # To use few-shot, set: prompt_builder = build_prompt_few_shot\n","    prompt_builder = build_prompt_one_shot  # change as needed\n","\n","    output_file = \"fold_wise_predictions_one_shot.jsonl\"\n","    llm_pipeline = load_llm_qwen(use_gpu=True)\n","\n","    with open(output_file, \"w\") as outfile:\n","        for fold_num, (train_df, test_df) in enumerate(folds, start=1):\n","            logging.debug(f\"Processing Fold {fold_num}\")\n","            test_df = test_df.groupby(\"Date\").first().reset_index()\n","            for idx, row in test_df.iterrows():\n","                pred_binary, pred_word = predict_movement(llm_pipeline, row, prompt_builder)\n","                record = {\n","                    \"Date\": row[\"Date\"].isoformat(),\n","                    \"Fold\": fold_num,\n","                    \"Daily_Return\": row.get(\"Daily_Return\", None),\n","                    \"Daily_Return_lag1\": row.get(\"Daily_Return_lag1\", None),\n","                    \"Ground_Truth\": row.get(\"Movement\", None),\n","                    \"Model_Prediction\": pred_binary\n","                }\n","                outfile.write(json.dumps(record) + \"\\n\")\n","                outfile.flush()\n","                logging.debug(f\"Fold {fold_num}, Date {row['Date'].strftime('%Y-%m-%d')}: Prediction={pred_word} (binary={pred_binary})\")\n","\n","    logging.info(f\"Done! Wrote predictions for each fold to '{output_file}' in real time.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"v6PnhSBdl3RE"},"execution_count":null,"outputs":[]}]}